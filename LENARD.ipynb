{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the predictions and seperate between training1, training2, and testing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "DATA_DIR_NAME = \"researchData\"\n",
    "DATA_COL_NAME = \"Electricity:Facility [kW](Hourly)\"\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import pmdarima as pm\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import warnings\n",
    "import time\n",
    " \n",
    "DATA_DIR_NAME = \"input\"\n",
    "DATA_COL_NAME = \"Electricity:Facility [kW](Hourly)\"\n",
    "\n",
    "TESLA_DATA_DIR_NAME = \"teslaData\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# LSTM Code below derived from https://machinelearningmastery.com/\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = concat(columns, axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    " \n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    " \n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    " \n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,0]\n",
    "\n",
    "\n",
    "#LENARD MODEL\n",
    "dir_path = os.getcwd()\n",
    "directory = os.path.join(dir_path, DATA_DIR_NAME)\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        \n",
    "        print(file)\n",
    "\n",
    "        df = pd.read_csv(DATA_DIR_NAME + '/' + file)\n",
    "        \n",
    "        full = df[DATA_COL_NAME]\n",
    "        split_point = int(len(full) * .8)\n",
    "        \n",
    "        training_learners = full[:split_point]\n",
    "        testing_learners = full[split_point:]\n",
    "        \n",
    "        training_lenard = testing_learners[:int(len(testing_learners) * .8)]\n",
    "        testing_lenard  = testing_learners[int(len(testing_learners) * .8):]\n",
    "\n",
    "        \n",
    "        # ARIMA\n",
    "        arima_model = pm.auto_arima(training_learners, start_p=1, start_q=1,\n",
    "                          test='adf',       # use adftest to find optimal 'd'\n",
    "                          max_p=3, max_q=3, # maximum p and q\n",
    "                          m=24,              # frequency of series\n",
    "                          d=0,           # let model determine 'd'\n",
    "                          seasonal=False,   # Seasonality\n",
    "                          start_P=0, \n",
    "                          D=0, \n",
    "                          trace=True,\n",
    "                          error_action='ignore',  \n",
    "                          suppress_warnings=True, \n",
    "                          stepwise=True,\n",
    "                          n_jobs=-1)\n",
    "        \n",
    "        arima_model = ARIMA(training_learners, order=arima_model.order)\n",
    "        arima_model_fit = arima_model.fit(disp=0)   \n",
    "\n",
    "        arima_yhat = arima_model_fit.predict(start=0, end=len(testing_learners) - 1)\n",
    "        \n",
    "        \n",
    "        # Holt Winters\n",
    "        trend = [\"add\", \"mul\"]\n",
    "        damped = [True, False]\n",
    "        seasonal = [\"add\", \"mul\"]\n",
    "        seasonal_periods = [24, 168]\n",
    "        \n",
    "        best_mse = 100\n",
    "        \n",
    "        for trend_x in trend:\n",
    "            for damped_x in damped:\n",
    "                for seasonal_x in seasonal:\n",
    "                    for seasonal_periods_x in seasonal_periods:\n",
    "                        hw_model = ExponentialSmoothing(training_learners,\n",
    "                                                        trend=trend_x,\n",
    "                                                        seasonal=seasonal_x,\n",
    "                                                        seasonal_periods=seasonal_periods_x, \n",
    "                                                        damped=damped_x)   \n",
    "                        \n",
    "                        hw_model_fit = hw_model.fit()\n",
    "                        \n",
    "                        new_hw_yhat = hw_model_fit.predict(start=0, end=len(testing_learners) - 1)\n",
    "                        \n",
    "                        hw_mse = mean_squared_error(new_hw_yhat, testing_learners)\n",
    "                        \n",
    "                        if(hw_mse < best_mse):\n",
    "                            hw_yhat = new_hw_yhat\n",
    "                            best_mse = hw_mse    \n",
    "    \n",
    "        \n",
    "        # LSTM\n",
    "        lstm_split_point = int(len(full) * .2)\n",
    "        \n",
    "        diff_values = difference(full, 1)\n",
    "\n",
    "        supervised = timeseries_to_supervised(diff_values, 1)\n",
    "        supervised_values = supervised.values\n",
    "    \n",
    "        lstm_train, lstm_test = supervised_values[0:-lstm_split_point:], supervised_values[-lstm_split_point:]\n",
    " \n",
    "        # transform the scale of the data\n",
    "        scaler, train_scaled, test_scaled = scale(lstm_train, lstm_test)\n",
    " \n",
    "        # fit the model\n",
    "        lstm_model = fit_lstm(train_scaled, 1, 10, 2)\n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "        lstm_model.predict(train_reshaped, batch_size=1)\n",
    " \n",
    "        # walk-forward validation on the test data\n",
    "        lstm_yhat = list()\n",
    "        for i in range(len(test_scaled)):\n",
    "            # make one-step forecast\n",
    "            X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "            yhat = forecast_lstm(lstm_model, 1, X)\n",
    "            # invert scaling\n",
    "            yhat = invert_scale(scaler, X, yhat)\n",
    "            # invert differencing\n",
    "            yhat = inverse_difference(df[DATA_COL_NAME].values, yhat, len(test_scaled)+1-i)\n",
    "            # store forecast\n",
    "            lstm_yhat.append(yhat)\n",
    "            \n",
    "            \n",
    "        # Import Tesla predictions from matlab\n",
    "#         tesla_df = pd.read_csv(TESLA_DATA_DIR_NAME + '/' + file)\n",
    "#         tesla_yhat = tesla_df.index\n",
    "\n",
    "        # Persistence\n",
    "        persistence_yhat = full[split_point - 24:len(full) - 24]\n",
    "        \n",
    "        split_point = int(len(arima_yhat) * .8)\n",
    "    \n",
    "        train_house = np.array([arima_yhat[:split_point], # Training data\n",
    "                                hw_yhat[:split_point],\n",
    "                                lstm_yhat[:split_point],\n",
    "                                persistence_yhat[:split_point]])\n",
    "    \n",
    "        train_label = np.array(training_lenard)  # Training validation data\n",
    "    \n",
    "        test_house = np.array([arima_yhat[split_point:],  # Testing data\n",
    "                               hw_yhat[split_point:],\n",
    "                               lstm_yhat[split_point:],\n",
    "                               persistence_yhat[split_point:]])\n",
    "        \n",
    "        test_label = np.array(testing_lenard)\n",
    "        \n",
    "        x = np.array([[1]])\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(400, input_shape=(1,), activation='relu'),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "            # The wrapper is used to import the real training and validation data to the custom loss function\n",
    "        def custom_loss_wrapper(curr_house, curr_label) :\n",
    "    \n",
    "            # Uses the imported data to apply the outputted weights and use the mse between the real values and \n",
    "            # the summed prediction as the loss\n",
    "            def custom_loss(y_actual, y_predicted) :\n",
    "            \n",
    "                new_arima = curr_house[0] * y_predicted[0][0]\n",
    "                new_holt = curr_house[1] * y_predicted[0][1]\n",
    "                new_lstm = curr_house[2] * y_predicted[0][2]\n",
    "                new_persistence = curr_house[3] * y_predicted[0][3]\n",
    "                #new_tesla = curr_house[4] * y_predicted[0][4]\n",
    "    \n",
    "                new = tf.math.add(new_arima, new_holt)\n",
    "                new = tf.math.add(new, new_lstm)\n",
    "                new = tf.math.add(new, new_persistence)\n",
    "                #new = tf.math.add(new, new_tesla)\n",
    "            \n",
    "                return K.mean(K.square(new - curr_label))\n",
    "\n",
    "            return custom_loss\n",
    "    \n",
    "    \n",
    "        model.compile(optimizer='adam', \n",
    "                      loss=custom_loss_wrapper(tf.Variable(train_house, dtype=tf.float32), \n",
    "                                               tf.Variable(train_label, dtype=tf.float32)))\n",
    "    \n",
    "        # Trains the model using the first 80% of the learner's predictions\n",
    "        model.fit(x, x, epochs=500, verbose=0)\n",
    "    \n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=custom_loss_wrapper(tf.Variable(test_house, dtype=tf.float32), \n",
    "                                               tf.Variable(test_label, dtype=tf.float32)))\n",
    "    \n",
    "        # Calculates the mse between the actual values and the prediction found using the calculated weights\n",
    "        test_loss = model.evaluate(x, x, verbose=2)\n",
    "\n",
    "        print('\\nTest loss:', test_loss)\n",
    "    \n",
    "#         mse_list.append(test_loss)\n",
    "#         weights.append(model.predict(x)[0])\n",
    "        print(model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
